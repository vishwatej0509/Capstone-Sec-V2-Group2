{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9481431,"sourceType":"datasetVersion","datasetId":5767320},{"sourceId":9481473,"sourceType":"datasetVersion","datasetId":5767352}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torchvision.transforms as tt\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets.utils import download_url\nfrom torchvision.datasets import ImageFolder\n\nimport random\n\nimport copy\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport tarfile\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random\n%matplotlib inline\n\nfrom sklearn.metrics import f1_score\n\nfrom tqdm import tqdm\nimport wandb\n\nfrom glob import glob\n\nimport cv2 \nimport time\nfrom datetime import timedelta, datetime\nfrom PIL import Image\nimport PIL\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'\n\n\n# Accelerate parts\nfrom accelerate import Accelerator, notebook_launcher # main interface, distributed launcher\nfrom accelerate.utils import set_seed # reproducability across devices","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-08T07:07:11.543621Z","iopub.execute_input":"2024-10-08T07:07:11.544051Z","iopub.status.idle":"2024-10-08T07:07:20.964920Z","shell.execute_reply.started":"2024-10-08T07:07:11.544007Z","shell.execute_reply":"2024-10-08T07:07:20.963679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED_VAL = 7\nN_CLASSES_main = 1\nN_CLASSES_js = 30\n\n\n\ndef seed_everything(seed: int):\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = True\n    \nseed_everything(SEED_VAL)\n\norder = torch.randint(low = 1, \n                      high = 31,\n                      size = ()) \nprint(order.item())","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:20.967245Z","iopub.execute_input":"2024-10-08T07:07:20.967869Z","iopub.status.idle":"2024-10-08T07:07:21.018481Z","shell.execute_reply.started":"2024-10-08T07:07:20.967823Z","shell.execute_reply":"2024-10-08T07:07:21.017135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensures that if we rerun the same code, the random operations (shuffling, data splits, etc.) will give the exact same result, allowing for consistent experiments.","metadata":{}},{"cell_type":"code","source":"class Custom_dataset_RainDS(Dataset):\n    def __init__(self, \n                 base_folder_path: str,\n                 transforms = None,\n                 mode: str = \"train\"):\n        self.base_folder_path = base_folder_path\n        self.transforms = transforms\n        self.mode = mode\n        \n        fPath_GT = os.path.join(base_folder_path, \"gt\")\n        fPath_rdrop = os.path.join(base_folder_path, \"raindrop\")\n        fPath_rstreak= os.path.join(base_folder_path, \"rainstreak\")\n        fPath_rdrop_rstreak= os.path.join(base_folder_path, \"rainstreak_raindrop\")\n        \n        files_list_GT = glob(os.path.join(fPath_GT, \"*.png\"))\n        files_list_rdrop = glob(os.path.join(fPath_rdrop, \"*.png\"))\n        files_list_rstreak = glob(os.path.join(fPath_rstreak, \"*.png\"))\n        files_list_rdrop_rstreak = glob(os.path.join(fPath_rdrop_rstreak, \"*.png\"))\n        \n        files_list_GT.sort()\n        files_list_rdrop.sort()\n        files_list_rstreak.sort()\n        files_list_rdrop_rstreak.sort()\n        \n        self.data = []\n        master_SSL_file_list = files_list_GT + files_list_rdrop + files_list_rstreak + files_list_rdrop_rstreak\n        print(\"NUmber of images : \", len(master_SSL_file_list))\n        \n        for fname in master_SSL_file_list:\n            # 0 : 0 degrees\n            # 1 : 90 degrees\n            # 2 : 180 degrees\n            # 3 : 270 degrees\n            for i in range(4):\n                self.data.append([fname, i])\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        fname, class_id = self.data[idx]\n        img = cv2.imread(fname) \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if class_id == 1:\n            img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n        if class_id == 2:\n            img = cv2.rotate(img, cv2.ROTATE_180)\n        if class_id ==3:\n            img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n        \n        if self.transforms:\n            img = self.transforms(img)\n            \n        return {\"img\": img,\n                \"class_id\": class_id}","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:21.020388Z","iopub.execute_input":"2024-10-08T07:07:21.020885Z","iopub.status.idle":"2024-10-08T07:07:21.039795Z","shell.execute_reply.started":"2024-10-08T07:07:21.020831Z","shell.execute_reply":"2024-10-08T07:07:21.038513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This class defines how to load and process the dataset.\nReading images from specific directories (like \"raindrop\", \"rainstreak\", \"gt\").\nStoring the file paths and class IDs for each image.\nRotating images by 0째, 90째, 180째, and 270째 to create augmented versions.","metadata":{}},{"cell_type":"code","source":"stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\nimage_transforms = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(*stats, inplace=True),\n        transforms.Resize((224, 224), interpolation=PIL.Image.BILINEAR)\n    ])\n\ntrain_ds =  Custom_dataset_RainDS(base_folder_path = \"/kaggle/input/rainds/RainDS/RainDS_real/train_set\", \n                                      transforms = image_transforms,\n                                      mode = \"train\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:21.043171Z","iopub.execute_input":"2024-10-08T07:07:21.043690Z","iopub.status.idle":"2024-10-08T07:07:21.144335Z","shell.execute_reply.started":"2024-10-08T07:07:21.043637Z","shell.execute_reply":"2024-10-08T07:07:21.142954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset images are converted into PyTorch tensors, normalized (using predefined mean and std values), and resized to 224x224 pixels. These steps are essential for feeding the images into deep learning models.","metadata":{}},{"cell_type":"code","source":"idx = 0\nimg, class_id = train_ds[idx][\"img\"], train_ds[idx][\"class_id\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:21.145867Z","iopub.execute_input":"2024-10-08T07:07:21.146374Z","iopub.status.idle":"2024-10-08T07:07:21.415148Z","shell.execute_reply.started":"2024-10-08T07:07:21.146320Z","shell.execute_reply":"2024-10-08T07:07:21.413984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(img.shape)\nprint(class_id)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:21.416766Z","iopub.execute_input":"2024-10-08T07:07:21.417282Z","iopub.status.idle":"2024-10-08T07:07:21.423973Z","shell.execute_reply.started":"2024-10-08T07:07:21.417229Z","shell.execute_reply":"2024-10-08T07:07:21.422801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def denormalize(images, means, stds):\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef get_img_by_idx(idx):\n    img, _ = train_ds[idx][\"img\"], train_ds[idx][\"class_id\"]\n    img = torch.unsqueeze(img,0)\n    img = denormalize(img, *stats)\n    img = torch.squeeze(img, 0)\n    img = img.permute(1,2,0).numpy()\n    return img","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:21.425908Z","iopub.execute_input":"2024-10-08T07:07:21.426346Z","iopub.status.idle":"2024-10-08T07:07:21.437177Z","shell.execute_reply.started":"2024-10-08T07:07:21.426306Z","shell.execute_reply":"2024-10-08T07:07:21.435403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converts the normalized image values back to their original range (i.e., before normalization) for visualization. Since images were normalized, this function reverts the pixel values to make the image viewable.","metadata":{}},{"cell_type":"code","source":"\nprint(len(train_ds))  # Number of images after augmentation (should be original images * 4)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:21.439482Z","iopub.execute_input":"2024-10-08T07:07:21.440000Z","iopub.status.idle":"2024-10-08T07:07:21.453058Z","shell.execute_reply.started":"2024-10-08T07:07:21.439938Z","shell.execute_reply":"2024-10-08T07:07:21.451258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axarr = plt.subplots(4,6, figsize = (10,5))\nfor i in range(6):\n    axarr[0,i].imshow(get_img_by_idx(4*i))\n    axarr[1,i].imshow(get_img_by_idx(4*i+1))\n    axarr[2,i].imshow(get_img_by_idx(4*i+2))\n    axarr[3,i].imshow(get_img_by_idx(4*i+3))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:21.455247Z","iopub.execute_input":"2024-10-08T07:07:21.455722Z","iopub.status.idle":"2024-10-08T07:07:27.001878Z","shell.execute_reply.started":"2024-10-08T07:07:21.455680Z","shell.execute_reply":"2024-10-08T07:07:27.000581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_rain_type_distribution(dataset):\n    rain_types = []  # Example rain type labels, assuming you have them\n    for i in range(len(dataset)):\n        img_path = dataset.data[i][0]  # Get the image path\n        # Here, categorize by file path or folder name\n        if \"raindrop\" in img_path:\n            rain_types.append(\"raindrop\")\n        elif \"rainstreak\" in img_path:\n            rain_types.append(\"rainstreak\")\n        elif \"rainstreak_raindrop\" in img_path:\n            rain_types.append(\"raindrop + streak\")\n        else:\n            rain_types.append(\"ground truth\")\n\n    # Plot the distribution\n    plt.figure(figsize=(6, 4))\n    plt.hist(rain_types, bins=len(set(rain_types)), rwidth=0.8, color='skyblue')\n    plt.title(\"Rain Type Distribution\")\n    plt.xlabel(\"Rain Type\")\n    plt.ylabel(\"Number of Samples\")\n    plt.show()\n\n# Call the function to analyze rain type distribution\nplot_rain_type_distribution(train_ds)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:07:27.005958Z","iopub.execute_input":"2024-10-08T07:07:27.006968Z","iopub.status.idle":"2024-10-08T07:07:27.441797Z","shell.execute_reply.started":"2024-10-08T07:07:27.006901Z","shell.execute_reply":"2024-10-08T07:07:27.440468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot generated will show the number of images in each rain type category, which helps in understanding the distribution of rain types in the dataset (e.g., whether the dataset is balanced or skewed toward certain types of rain).","metadata":{}},{"cell_type":"code","source":"def load_images_to_memory(dataset, num_samples=100):\n    # Load a subset of images into memory\n    images = []\n    for i in range(num_samples):\n        img_path = dataset.data[i][0]\n        img = cv2.imread(img_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        images.append(img_rgb)\n    return images\n\ndef plot_pixel_intensity_distribution(images):\n    # Flatten and collect pixel values from all images\n    pixel_values = [img.flatten() for img in images]\n    pixel_values = np.concatenate(pixel_values)  # Flatten into a 1D array\n\n    # Plot histogram\n    plt.hist(pixel_values, bins=50, color='skyblue')\n    plt.title(\"Pixel Intensity Distribution\")\n    plt.xlabel(\"Pixel Intensity\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n# Load images into memory\nimages_in_memory = load_images_to_memory(train_ds, num_samples=100)\n\n# Plot pixel intensity distribution for the preloaded images\nplot_pixel_intensity_distribution(images_in_memory)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T08:12:25.856362Z","iopub.execute_input":"2024-10-08T08:12:25.857668Z","iopub.status.idle":"2024-10-08T08:12:34.307689Z","shell.execute_reply.started":"2024-10-08T08:12:25.857617Z","shell.execute_reply":"2024-10-08T08:12:34.306394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" By plotting the pixel intensity distribution, this code helps to understand the brightness and contrast characteristics of the dataset. It can highlight whether the images are mostly dark, bright, or well-balanced. Based on this inference, we can try to adjust the brightness/contrast of images to make it similar across all images before training the model.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Load a subset of images into memory\ndef load_images_to_memory(dataset, num_samples=100):\n    images = []\n    for i in range(num_samples):\n        img_path = dataset.data[i][0]\n        img = cv2.imread(img_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        images.append(img_rgb)\n    return images\n\n# Apply CLAHE for image enhancement\ndef apply_clahe_to_images(images):\n    # Create a CLAHE object\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    \n    # Apply CLAHE to each image\n    enhanced_images = []\n    for img in images:\n        # Convert RGB image to LAB color space\n        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        \n        # Apply CLAHE to the L-channel\n        l_clahe = clahe.apply(l)\n        \n        # Merge the channels back\n        lab_clahe = cv2.merge((l_clahe, a, b))\n        \n        # Convert back to RGB\n        img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n        enhanced_images.append(img_clahe)\n    \n    return enhanced_images\n\n# Display original and CLAHE-enhanced images side-by-side\ndef display_two_images(original_img, enhanced_img):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    axes[0].imshow(original_img)\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis('off')\n    \n    axes[1].imshow(enhanced_img)\n    axes[1].set_title(\"CLAHE-Enhanced Image\")\n    axes[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot pixel intensity distribution\ndef plot_pixel_intensity_distribution(images):\n    pixel_values = [img.flatten() for img in images]\n    pixel_values = np.concatenate(pixel_values)\n    \n    plt.hist(pixel_values, bins=50, color='skyblue')\n    plt.title(\"Pixel Intensity Distribution\")\n    plt.xlabel(\"Pixel Intensity\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n# Load images into memory\nimages_in_memory = load_images_to_memory(train_ds, num_samples=100)\n\n# Apply CLAHE for image enhancement\nenhanced_images = apply_clahe_to_images(images_in_memory)\n\n# Display just two images (original and CLAHE-enhanced)\ndisplay_two_images(images_in_memory[0], enhanced_images[0])\n\n# Plot pixel intensity distribution for the CLAHE-enhanced images\nplot_pixel_intensity_distribution(enhanced_images)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T08:10:50.704814Z","iopub.execute_input":"2024-10-08T08:10:50.705911Z","iopub.status.idle":"2024-10-08T08:11:01.296806Z","shell.execute_reply.started":"2024-10-08T08:10:50.705850Z","shell.execute_reply":"2024-10-08T08:11:01.295362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageEnhance, Image\n\ndef adjust_image_attributes(images, sharpness_factor=1.5, brightness_factor=1.2, contrast_factor=1.3):\n    # Apply sharpness, brightness, and contrast adjustments\n    adjusted_images = []\n    \n    for img in images:\n        # Convert to PIL Image for adjustments\n        pil_img = Image.fromarray(img)\n        \n        # Adjust sharpness\n        enhancer = ImageEnhance.Sharpness(pil_img)\n        pil_img = enhancer.enhance(sharpness_factor)\n        \n        # Adjust brightness\n        enhancer = ImageEnhance.Brightness(pil_img)\n        pil_img = enhancer.enhance(brightness_factor)\n        \n        # Adjust contrast\n        enhancer = ImageEnhance.Contrast(pil_img)\n        pil_img = enhancer.enhance(contrast_factor)\n        \n        # Convert back to NumPy array (RGB)\n        adjusted_img = np.array(pil_img)\n        adjusted_images.append(adjusted_img)\n    \n    return adjusted_images\n\n# Load images into memory\nimages_in_memory = load_images_to_memory(train_ds, num_samples=100)\n\n# Apply CLAHE for image enhancement\nenhanced_images = apply_clahe_to_images(images_in_memory)\n\n# Apply sharpness, brightness, and contrast adjustments\nadjusted_images = adjust_image_attributes(enhanced_images, sharpness_factor=1.5, brightness_factor=1.2, contrast_factor=1.3)\n\n# Plot pixel intensity distribution for the adjusted images\nplot_pixel_intensity_distribution(adjusted_images)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:19:46.874249Z","iopub.execute_input":"2024-10-08T07:19:46.875823Z","iopub.status.idle":"2024-10-08T07:20:04.401896Z","shell.execute_reply.started":"2024-10-08T07:19:46.875772Z","shell.execute_reply":"2024-10-08T07:20:04.400568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageEnhance, Image\n\ndef adjust_image_attributes(images, sharpness_factor=1.2, brightness_factor=1.1, contrast_factor=1.1):\n    # Apply sharpness, brightness, and contrast adjustments\n    adjusted_images = []\n    \n    for img in images:\n        # Convert to PIL Image for adjustments\n        pil_img = Image.fromarray(img)\n        \n        # Adjust sharpness\n        enhancer = ImageEnhance.Sharpness(pil_img)\n        pil_img = enhancer.enhance(sharpness_factor)\n        \n        # Adjust brightness\n        enhancer = ImageEnhance.Brightness(pil_img)\n        pil_img = enhancer.enhance(brightness_factor)\n        \n        # Adjust contrast\n        enhancer = ImageEnhance.Contrast(pil_img)\n        pil_img = enhancer.enhance(contrast_factor)\n        \n        # Convert back to NumPy array (RGB)\n        adjusted_img = np.array(pil_img)\n        adjusted_images.append(adjusted_img)\n    \n    return adjusted_images\n\n# Load images into memory\nimages_in_memory = load_images_to_memory(train_ds, num_samples=100)\n\n# Apply CLAHE for image enhancement\nenhanced_images = apply_clahe_to_images(images_in_memory)\n\n# Apply more subtle sharpness, brightness, and contrast adjustments\nadjusted_images = adjust_image_attributes(enhanced_images, sharpness_factor=1.2, brightness_factor=1.1, contrast_factor=1.1)\n\n# Plot pixel intensity distribution for the adjusted images\nplot_pixel_intensity_distribution(adjusted_images)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:28:17.207442Z","iopub.execute_input":"2024-10-08T07:28:17.207979Z","iopub.status.idle":"2024-10-08T07:28:33.887104Z","shell.execute_reply.started":"2024-10-08T07:28:17.207934Z","shell.execute_reply":"2024-10-08T07:28:33.885888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageEnhance, Image\n\ndef adjust_image_attributes(images, sharpness_factor=1.01, brightness_factor=1.005, contrast_factor=0.98):\n    # Apply sharpness, brightness, and contrast adjustments\n    adjusted_images = []\n    \n    for img in images:\n        # Convert to PIL Image for adjustments\n        pil_img = Image.fromarray(img)\n        \n        # Adjust sharpness\n        enhancer = ImageEnhance.Sharpness(pil_img)\n        pil_img = enhancer.enhance(sharpness_factor)\n        \n        # Slightly adjust brightness\n        enhancer = ImageEnhance.Brightness(pil_img)\n        pil_img = enhancer.enhance(brightness_factor)\n        \n        # Keep contrast at slightly reduced\n        enhancer = ImageEnhance.Contrast(pil_img)\n        pil_img = enhancer.enhance(contrast_factor)\n        \n        # Convert back to NumPy array (RGB)\n        adjusted_img = np.array(pil_img)\n        adjusted_images.append(adjusted_img)\n    \n    return adjusted_images\n\n# Load images into memory\nimages_in_memory = load_images_to_memory(train_ds, num_samples=100)\n\n# Apply CLAHE for image enhancement\nenhanced_images = apply_clahe_to_images(images_in_memory)\n\n# Apply very slight sharpness, brightness, and reduce contrast\nadjusted_images = adjust_image_attributes(enhanced_images, sharpness_factor=1.005, brightness_factor=1.0, contrast_factor=0.97)\n\n# Display just two images (before and after)\ndisplay_two_images(images_in_memory[0], adjusted_images[0])\n\n# Plot pixel intensity distribution for the adjusted images\nplot_pixel_intensity_distribution(adjusted_images)","metadata":{},"execution_count":null,"outputs":[]}]}